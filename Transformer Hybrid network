import tensorflow as tf
from tensorflow.keras import layers, Model

# Step 1: Depth-wise CNN for feature extraction
class DepthwiseCNN(tf.keras.layers.Layer):
    def __init__(self):
        super(DepthwiseCNN, self).__init__()
        self.conv1 = layers.Conv2D(64, (3, 3), activation='relu', padding='same', depthwise=True)
        self.conv2 = layers.Conv2D(64, (3, 3), activation='relu', padding='same', depthwise=True)

    def call(self, inputs):
        x = self.conv1(inputs)
        x = self.pool(x)
        x = self.conv2(x)
        x = self.pool(x)
        x = self.conv3(x)
        x = self.pool(x)
        return x  # Shape: (batch_size, height/8, width/8, 128)

# Step 2: Patch Embedding Layer
class PatchEmbedding(layers.Layer):
    def __init__(self, patch_size):
        super(PatchEmbedding, self).__init__()
        self.patch_size = patch_size

    def call(self, inputs):
        batch_size, height, width, channels = tf.shape(inputs)
        # Create patches
        patches = tf.image.extract_patches(
            images=inputs,
            sizes=[1, self.patch_size, self.patch_size, 1],
            strides=[1, self.patch_size, self.patch_size, 1],
            rates=[1, 1, 1, 1],
            padding='VALID'
        )
        # Flatten patches
        patches = tf.reshape(patches, [batch_size, -1, self.patch_size * self.patch_size * channels])
        return patches  # Shape: (batch_size, num_patches, patch_dim)

# Step 3: Positional Encoding Layer
class PositionalEncoding(layers.Layer):
    def __init__(self, num_patches, embedding_dim):
        super(PositionalEncoding, self).__init__()
        self.positional_embedding = self.add_weight("pos_embedding", shape=[1, num_patches, embedding_dim])

    def call(self, inputs):
        return inputs + self.positional_embedding

# Step 4: Feedforward Network for Relevance Scoring
class RelevanceScoringNetwork(layers.Layer):
    def __init__(self):
        super(RelevanceScoringNetwork, self).__init__()
        self.dense1 = layers.Dense(128, activation='relu')
        self.dense2 = layers.Dense(1, activation='linear')

    def call(self, inputs):
        x = self.dense1(inputs)
        scores = self.dense2(x)
        return scores  # Shape: (batch_size, num_patches, 1)

# Step 5: Adaptive Sparse Self Attention Block
class AdaptiveSparseSelfAttention(tf.keras.layers.Layer):
    def __init__(self, num_heads, key_dim, global_tokens=10):
        super(AdaptiveSparseSelfAttention, self).__init__()
        self.num_heads = num_heads
        self.key_dim = key_dim
        self.global_tokens = global_tokens

        # Dense layers for query, key, and value transformations
        self.query_dense = layers.Dense(num_heads * key_dim)
        self.key_dense = layers.Dense(num_heads * key_dim)
        self.value_dense = layers.Dense(num_heads * key_dim)

    def call(self, inputs, relevance_scores):
        # Step 1: Select top patches based on relevance scores
        top_patches_indices = tf.argsort(relevance_scores, direction='DESCENDING', axis=1)[:, :self.global_tokens]
        top_patches = tf.gather(inputs, top_patches_indices, batch_dims=1)

        # Step 2: Compute queries, keys, values
        query = self.query_dense(top_patches)
        key = self.key_dense(top_patches)
        value = self.value_dense(top_patches)

        # Step 3: Calculate attention scores
        attention_scores = tf.matmul(query, key, transpose_b=True)
        attention_scores = tf.nn.softmax(attention_scores, axis=-1)  # Normalize attention scores

        # Step 4: Apply attention to values
        attention_output = tf.matmul(attention_scores, value)

        return attention_output

# Step 6: Multi-Head Dilated Self-Attention Block
class MultiHeadDilatedSelfAttention(tf.keras.layers.Layer):
    def __init__(self, num_heads, key_dim, dilation_rate=2):
        super(MultiHeadDilatedSelfAttention, self).__init__()
        self.num_heads = num_heads
        self.key_dim = key_dim
        self.dilation_rate = dilation_rate

        # Dense layers for query, key, and value transformations
        self.query_dense = layers.Dense(num_heads * key_dim)
        self.key_dense = layers.Dense(num_heads * key_dim)
        self.value_dense = layers.Dense(num_heads * key_dim)

    def call(self, inputs):
        # Compute queries, keys, values
        query = self.query_dense(inputs)
        key = self.key_dense(inputs)
        value = self.value_dense(inputs)

        # Reshape for multi-head attention
        batch_size, num_patches, _ = tf.shape(inputs)
        query = tf.reshape(query, (batch_size, num_patches, self.num_heads, self.key_dim))
        key = tf.reshape(key, (batch_size, num_patches, self.num_heads, self.key_dim))
        value = tf.reshape(value, (batch_size, num_patches, self.num_heads, self.key_dim))

        # Dilated attention scores
        attention_scores = tf.matmul(query, key, transpose_b=True) / (self.key_dim ** 0.5)
        attention_scores = tf.nn.softmax(attention_scores, axis=-1)  # Normalize attention scores

        # Apply attention to values
        attention_output = tf.matmul(attention_scores, value)

        # Reshape back to original
        attention_output = tf.reshape(attention_output, (batch_size, num_patches, self.num_heads * self.key_dim))

        return attention_output

# Step 7: Combination Layer
class CombinationLayer(tf.keras.layers.Layer):
    def __init__(self):
        super(CombinationLayer, self).__init__()

    def call(self, sparse_output, dilated_output):
        # Combine outputs from sparse and dilated attention
        combined_output = sparse_output + dilated_output  # Element-wise addition
        return combined_output

# Step 8: Classification Block
class ClassificationBlock(tf.keras.layers.Layer):
    def __init__(self, num_classes):
        super(ClassificationBlock, self).__init__()
        self.dense1 = layers.Dense(128, activation='relu')
        self.dense2 = layers.Dense(num_classes, activation='softmax')

    def call(self, inputs):
        x = self.dense1(inputs)
        return self.dense2(x)

# Example Model Integration
class MyModel(tf.keras.Model):
    def __init__(self, patch_size, num_heads, key_dim, num_classes):
        super(MyModel, self).__init__()
        self.depthwise_cnn = DepthwiseCNN()
        self.patch_embedding = PatchEmbedding(patch_size)
        self.positional_encoding = PositionalEncoding(num_patches=196, embedding_dim=768)  # Example values
        self.relevance_network = RelevanceScoringNetwork()

        # Two blocks of adaptive sparse self-attention
        self.adaptive_sparse_attention1 = AdaptiveSparseSelfAttention(num_heads, key_dim)
        self.adaptive_sparse_attention2 = AdaptiveSparseSelfAttention(num_heads, key_dim)

        # Two blocks of multi-head dilated self-attention
        self.multi_head_dilated_attention1 = MultiHeadDilatedSelfAttention(num_heads, key_dim)
        self.multi_head_dilated_attention2 = MultiHeadDilatedSelfAttention(num_heads, key_dim)

        # Combination layer
        self.combination_layer = CombinationLayer()

        # Final adaptive sparse self-attention
        self.final_adaptive_sparse_attention = AdaptiveSparseSelfAttention(num_heads, key_dim)

        # Classification block
        self.classification_block = ClassificationBlock(num_classes)

    def call(self, inputs):
        x = self.depthwise_cnn(inputs)  # Extract features
        x = self.patch_embedding(x)  # Divide into patches
        x = self.positional_encoding(x)  # Apply positional encoding
        relevance_scores = self.relevance_network(x)  # Compute relevance scores

        # First adaptive sparse self-attention
        sparse_output1 = self.adaptive_sparse_attention1(x, relevance_scores)
        # Second adaptive sparse self-attention
        sparse_output2 = self.adaptive_sparse_attention2(sparse_output1, relevance_scores)

        # First multi-head dilated self-attention
        dilated_output1 = self.multi_head_dilated_attention1(sparse_output2)
        # Second multi-head dilated self-attention
        dilated_output2 = self.multi_head_dilated_attention2(dilated_output1)

        # Combine outputs
        combined_output = self.combination_layer(sparse_output2, dilated_output2)

        # Final adaptive sparse self-attention
        final_output = self.final_adaptive_sparse_attention(combined_output, relevance_scores)

        # Classification
        output = self.classification_block(final_output)
        return output



